I"H<h1 id="foundation-of-neural-networks">Foundation of Neural Networks</h1>

<p>History has it be it the Greek Mythology <strong>Pandora</strong> who was created out of clay and was the first mortal women, or <strong>Talos</strong> the giant bronze man who would circle the island of Crete 3 times a day and protect the island against pirates with rocks, or be it the <strong>Jewish writers</strong> who studied the Sefer Yetzirah to bring the golems to life, this shows us that humans have always wanted automation in life be it creating life or making machines.</p>

<p>Now the ideas that sparked the development of machines or machine’s that could think could have started way too long ago but the advancements in technology and the field of medicine, and other field’s lead to majority of the contribution’s happening in the 1600s and in the later/earlier centuries which caused a major leap in the field of thinking machines.</p>

<p>Now let’s get started and have a look at the timeline of AI, from the discovery of cell to Thinking Machines.</p>

<h2 id="the-events">The Events</h2>

<h3 id="the-year-1665">THE YEAR 1665</h3>
<hr class="separator" />

<h4 id="discovery-of-cell">Discovery of Cell</h4>

<p><strong>Robert Hooke</strong>, a philosopher/scientist, took a clear piece of cork and then proceeded to cut a small clean piece of it and then watched that piece of cork under a microscope, that he constructed for himself. The results of the experimentation were a lot different from what he had anticipated, he found that the cork is made up of many little compartments, like the compartments that monks inhabit, which made him call them cells. At the time the microscope was not so developed, which limited the magnification power and thus his observations.</p>

<p>It was later known that, what Robert Hooke had seen were not cells but cell walls of dead cells.</p>

<p>All of his research was published in a book called “Micrographia: or Some Physiological Descriptions of Minute Bodies Made by Magnifying Glasses” which has his observations of the various object under a microscope.</p>

<h3 id="the-year-1646-1716">THE YEAR 1646-1716</h3>
<hr class="separator" />

<h4 id="the-propositional-logic">The Propositional Logic</h4>

<p>According to <strong>Gottfried Leibniz</strong> a logician, mathematician, and philosopher, the complex human thought’s could be broken down into much simpler thoughts. Just as complex human sentences could be broken down into words and words could be broken down to just a mere collection of letters, he also said that if we represent the non-comlex thoughts with a symbol and then we can combine those symbols through rules and logical means to form all the Knowledge.</p>

<p>Note: <strong>Leibniz</strong> was one of the first to express logical statements like the conjunction, disjunction, etc.</p>

<h3 id="the-year-1674">THE YEAR 1674</h3>
<hr class="separator" />

<h4 id="window-to-minute-world">Window to minute world</h4>

<p><strong>Anton van Leeuwenhoek</strong>, a scientist, while working at a draper’s shop the circumstance’s there sparked an interest in him for microscopes and he started working as a lens maker. As a lens maker he created many different lenses and microscopes, some of them having higher Magnification Power.</p>

<p>He used these microscopes to escape into the world of minute organisms from the harsh reality of the world, he discovered the “animalcules”, red blood cells, etc. He too, shortly after Robert Hooke, saw the piece of cork under the microscope, his microscope being powerful than Hooke’s, enabled him to see small moving organisms in the cell and concluded that they were living organisms because they were motile.</p>

<p>Note: <strong>Anton van Leeuwenhoek</strong> was the first person to look at cellular life.</p>

<h3 id="the-year-1839">THE YEAR 1839</h3>
<hr class="separator" />

<h4 id="cell-theory">Cell Theory</h4>

<p><strong>Theodor Schwann</strong> and <strong>Matthias Jakob Schleiden</strong>, with regards to other scientists, proposed the cell theory. Matthias Jakob Schleiden and Theodor Schwann proposed that the structural and functional parts of plants and animals were a result of cells.</p>

<p>This was a major <strong>BREAKTHROUGH</strong>, until then it was hard to observe the animal tissue cause it was too fragile to work with making it susceptible to breaking.</p>

<h3 id="the-year-1863">THE YEAR 1863</h3>
<hr class="separator" />

<h4 id="the-first-neuron-model">The First Neuron Model</h4>

<p><strong>Otto Deiters</strong>, an anatomists, known for his microscopic research on the brain and spinal cord, developed a technique in which he used a combination of chromic acid and carmine red [ dye ]. After staining the tissue with a carmine red dye, or any other dye, and dipping it into the chromic acid solution, which hardened the delicate tissue making it less fragile and the stains helping for easier observation of neurons.</p>

<p>This method allowed him to see neurons, for the first time in the world, with great details. He gave the most extensive description of the neuron at that time. He discovered “axis cylinder” [ axons ] and “protoplasmic processes” [ dendrites ].</p>

<p>He concluded, after his research, that the “axis cylinder” and “protoplasmic processes” fuse to form a continuous system [“nervous system”].</p>

<h3 id="the-year-1873">THE YEAR 1873</h3>
<hr class="separator" />

<h4 id="improved-neuron-model">Improved Neuron Model</h4>

<p><strong>Camillo Golgi</strong>, biologists, and pathologists developed a method that would allow him to visualize the structure of the neuron with great detail more than that of Otto Deiters.</p>

<p>His method was similar to that of Otto Deiters, he would take the tissue sample to dip it in the chromic solution which would harden the tissue but instead using a dye for staining the neurons, he found that using silver salts such as silver nitrate would stain the tissue to more detail since the silver nitrate salt would react with the potassium dichromate to form remainings of silver chromate on the cell membrane and staining it black.</p>

<p>But it’s a fascinating method, because the reaction mentioned above would stain only a few neurons per sample otherwise Camillo Golgi would have been left with a black patch. His method is also known as the Golgi reaction or black reaction.</p>

<p>He too suggested that maybe neurons connect to form a continuous system.</p>

<h3 id="the-year-1887">THE YEAR 1887</h3>
<hr class="separator" />

<h4 id="neuron-connections">Neuron Connection’s</h4>

<p><strong>Santiago Ramón y Cajal</strong>, neuroscientists, and pathologists, using the same Golgi reaction/Black reaction stained the neurons. But his conclusion differed from that of the Camillo Golgi, he said that the neurons did not merge to form a continuous network, instead, the speculated ending’s made temporary contact with other neurons, making them working independently of other neurons, which was later confirmed with the invention of the electron microscope.</p>

<p>His conclusion also put an end to the reticluar theory.</p>

<h3 id="the-year-1891">THE YEAR 1891</h3>
<hr class="separator" />

<h4 id="neurone-doctrine">Neurone Doctrine</h4>

<p><strong>Heinrich Wilhelm Gottfried von Waldeyer-Hartz</strong>, an anatomist, published a paper called “<strong>Ueber einige neuere Forschungen im Gebiete der Anatomie des Centralnervensystems (Schluss aus No. 49.)</strong>” in which he used the conclusion of other scientists and deduced that neuron is the basic structural and functional unit of the nervous system. This led to the formation of the “Neuron Doctrine”.</p>

<h3 id="the-year-1930s">THE YEAR 1930s</h3>
<hr class="separator" />

<h4 id="mathematical-model-of-brain">Mathematical model of Brain</h4>

<p><strong>Nicolas Rashevsky</strong>, a theoretical physicist, mathematical biologists, mathematical biophysicist, and a theoretical biologists, in a way or other, suggested that due to the higher complexity of the biological processes, we needed methods of theoretical physics and mathematics to breakdown the complex processes such as cell division and neural activity into much simpler mechanisms.</p>

<p>Nicolas Rashevsky developed the first mathematical model of the neural networks.</p>

<h3 id="the-year-1936">THE YEAR 1936</h3>
<hr class="separator" />

<h4 id="the-turing-machine">The Turing Machine</h4>

<p>The Turing machine, a machine that would simulate any complex logic, was invented in the year 1936 by <strong>Alan Turing</strong>.</p>

<h3 id="the-year-1942">THE YEAR 1942</h3>
<hr class="separator" />

<h4 id="first-artificial-neuron">First Artificial Neuron</h4>

<p><strong>Walter Pitts</strong>, a logician, and <strong>Warren McCulloch</strong>, neurophysiologists and cybernetician, worked together in the house of McCulloch. Both being influenced by Gottfried Leibniz they considered what Leibniz had speculated that, can all of the human thoughts broken down into much simpler ones where each one of them will be represented with a symbol and then with a set of rules we can combine the symbols to form all knowledge.</p>

<p>They were also influenced by the works of <strong>Nicolas Rashevsky</strong>, who presented the complex biological processes in the form of numbers and also credited with the creator of the first neural network, as they said in an article published in the year 1943 in “Bulletin of Mathematical Biophysics” board.</p>

<p>In the same year, both of them together published a paper on “Neural Network” named “A Logical Calculus of Ideas Immanent in Nervous Activity” talking about the relation of neurons can be treated with propositional logic. Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated through propositional logic</p>

<p>This article was influential for many in the field of neural networks and Artificial Intelligence.</p>

<h3 id="the-year-1947">THE YEAR 1947</h3>
<hr class="separator" />

<h4 id="learning">Learning</h4>

<p><strong>Walter Pitts</strong> and <strong>Warren McCulloch</strong> were able to create an “Artificial Neuron” but they did not speculate the possibilities of making it perform like a brain in various ways, since they were trying to represent the working of the brain or neurons in terms of propositional logic.</p>

<p>The great <strong>Alan Turing</strong> suggested that a randomly connected network of “artificial neurons” could be made to learn about a certain task.</p>

<h3 id="the-year-1948">THE YEAR 1948</h3>
<hr class="separator" />

<h4 id="infants-brain">Infant’s Brain</h4>

<p>The great <strong>Alan Turing</strong> suggested a new concept which he called the “unorganized machine” and was a theoretical model of an infant’s brain, and he also suggested a way to make it train and learn something about a specific task.</p>

<h3 id="the-year-1949">THE YEAR 1949</h3>
<hr class="separator" />

<h4 id="stimuli-response">Stimuli-Response</h4>

<p><strong>Edward Thorndike</strong>, psychologist, in simpler terms said that a certain response for a stimulus would become a habit when the stimuli-response pair is regular, or a certain response for a stimulus would become weakened when regularity is not present.</p>

<p>An example for it would be: <strong>Pavlov’s Dog Experiment</strong> [Pavlov’s experiment].</p>

<p>He also speculated that a certain habit would strengthen certain synapses or disintegration of that habit would weaken it.</p>

<h3 id="the-year-1949-1">THE YEAR 1949</h3>
<hr class="separator" />

<h4 id="hebbs-rule">Hebb’s Rule</h4>

<p><strong>Donald Hebb</strong>, neuropsychologists, published a book “The Organization of Behaviour” in which noted, now called as Hebb’s rule, in simpler terms saying that the neurons involved in a particular activity would somehow be evolved and become strengthened, and this is one of the fundamental operations necessary for learning and memory.</p>

<h3 id="the-year-1949-2">THE YEAR 1949</h3>
<hr class="separator" />

<h4 id="the-turing-test">The Turing Test</h4>

<p><strong>Alan Turing</strong> suggested the “Turing test” for testing a machine’s Intelligence.</p>

<h3 id="the-year-1951">THE YEAR 1951</h3>
<hr class="separator" />

<h4 id="snarc">SNARC</h4>

<p><strong>Marvin Minsky</strong>, a cognitive scientist, and <strong>Dean Edmonds</strong>, then a physics graduate, built the first neural computer called SNARC [ Stochastic Neural Analog Reinforcement Calculator ]. It was the first neural net machine [40 Hebb Synapses] ever built and is a pioneer in the field of AI, it was also successful in solving a maze.</p>

<h3 id="the-year-1954">THE YEAR 1954</h3>
<hr class="separator" />

<h4 id="first-neural-net-simulation">First Neural Net Simulation</h4>

<p>Shortly after Turing’s death, <strong>B.G. Farley</strong> and <strong>W.A. Clark</strong> ran the first simulation of a 128 neuron [“brain”] to recognize simple patterns and were successful with it. They also found the destruction of 10% of neurons of the neural network did not affect it just like the brain’s in the real world were damage to some number of neurons would not affect the working of the brain.</p>

<h3 id="the-year-1957">THE YEAR 1957</h3>
<hr class="separator" />

<h4 id="perceptron">Perceptron</h4>

<p><strong>Frank Rosenblatt</strong>, psychologists, improved the <strong>McCulloch-Pitts</strong> artificial neuron, using the founding’s of other scientists, and went on to create his version of artificial neuron called the “Perceptron”.</p>

<p>The difference between the “artificial neuron” and the “Perceptron” was the “weights”. The “Perceptron” was designed according to the Hebbian rule where a certain synapse would either strengthen or weakened based on the regularity of the activity causing the same neurons to fire.</p>

<h3 id="the-year-1969">THE YEAR 1969</h3>
<hr class="separator" />

<h4 id="limitations-of-perceptron">Limitations of Perceptron</h4>

<p><strong>Marvin Minsky</strong> and <strong>Seymour Papert</strong> wrote a paper called “Perceptrons: An Introduction to Computational Geometry”, in which they talked about the limitations of the perceptron noting that it could only solve linearly separable problems, causing negativity towards the development of AI. This caused a lack of research in the 1980s.</p>

<h3 id="the-year-1974">THE YEAR 1974</h3>
<hr class="separator" />

<h4 id="learning-method">Learning Method</h4>

<p><strong>Paul John Werbos</strong>, a social scientist and machine learning pioneer, known for his 1974 dissertation, in which he described a procedure called “Backpropagation” to train neural networks.</p>

<p>This discovery again sparked the excitement in the field of AI.</p>

<p>Note: Backpropagation was derived by multiple researchers in the early 60’s and implemented to run on computers as early as 1970 by Seppo Linnainmaa. But Paul Werbos was first one to suggest to apply it to multi neural networks.</p>

<h3 id="the-year-1986">THE YEAR 1986</h3>
<hr class="separator" />

<h4 id="multi-layered-learning">Multi-layered learning</h4>

<p><strong>David Rumelhart</strong>, psychologists, was one of the first people to apply the backpropagation to multi-layered neural networks. But his research does not cite the earlier authors of the procedure called “Backpropagation”.</p>

<h3 id="the-year-1970s--1980s">THE YEAR 1970s -1980s</h3>
<hr class="separator" />

<h4 id="parallel-distributed-processing">Parallel Distributed Processing</h4>

<p>PDP-8 facts</p>

<p>The neuron’s in the brain work in a parallel way, that is the brain works in parallel, processing information in parallel or multi -tasking way. For this the Parallel Distributed Processing was developed, the eight facts of the Parallel Distributed Processing :</p>

<ul>
  <li>
    <p>A set of processing units, represented by a set of integers.</p>
  </li>
  <li>
    <p>An activation for each unit, represented by a vector of time-dependent functions</p>
  </li>
  <li>
    <p>An output function for each unit, represented by a vector of functions on the activations.</p>
  </li>
  <li>
    <p>A pattern of connectivity among units, represented by a matrix of real numbers indicating connection strength.</p>
  </li>
  <li>
    <p>A propagation rule spreading the activations via the connections, represented by a function on the output of the units.</p>
  </li>
  <li>
    <p>An activation rule for combining inputs to a unit to determine its new activation, represented by a function on the current activation and propagation.</p>
  </li>
  <li>
    <p>A learning rule for modifying connections based on experience, represented by a change in the weights based on any number of variables.</p>
  </li>
  <li>
    <p>An environment that provides the system with experience, represented by sets of activation vectors for some subset of the units.</p>
  </li>
</ul>

<h3 id="current-time">CURRENT TIME</h3>
<hr class="separator" />

<h4 id="advancements">Advancements</h4>

<p>Over the course of years a lot has been contributed to the field of AI. New methods have been found to make neural networks train and learn new tasks, New neural network structures have been found. AI has evolved a lot, AI is able to drive cars with a bit of human assistance, it can beat humans in various tasks and games, recognize faces, replicate voices, perform simpler surgeries with human assistance and much more.</p>

<p>But there is a lot to this field, let’s see what the future beholds.</p>

<h2 id="references">References</h2>
<hr class="separator" />

<ul>
  <li><strong>Micrographia: or Some Physiological Descriptions of Minute Bodies Made by Magnifying Glasses</strong> by Robert Hooke</li>
  <li><a href="https://en.wikipedia.org/wiki/Neural_network#History"> Wikipedia </a></li>
  <li><strong>Computing Machinery and Intelligence</strong> by Alan Turing.</li>
  <li>Minsky Papert “<strong>Perceptrons</strong>”</li>
  <li><strong>THE PERCEPTRON: A PROBABILISTIC MODEL FOR INFORMATION STORAGE AND ORGANIZATION IN THE BRAIN</strong> by Frank Rosenblatt</li>
  <li><strong>A logical calculus of ideas immanent in nervous activity</strong> by Walter Pitts and Warren McCulloch.</li>
</ul>
:ET